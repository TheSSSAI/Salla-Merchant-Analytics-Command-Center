"risk_id","risk_category","risk_description","probability","impact","risk_score","priority_level","affected_tasks","root_cause","mitigation_strategy","contingency_plan","monitoring_trigger","owner","due_date","status"
"RISK-001","Technical","Vercel Serverless Function execution timeout during historical data synchronization (WI-DATA-002). Importing 12-24 months of Salla data involves thousands of API calls, which will exceed the 10-60 second execution limit of standard serverless functions.","5","5","25","Critical","WI-DATA-002, WS-SALLA-SYNC","Architectural mismatch between long-running ETL processes and ephemeral serverless runtime constraints.","Implement a recursive pagination pattern using Upstash QStash. The worker processes one page of data, saves state to Redis, and enqueues the next page fetch as a new event, keeping individual execution times under 10 seconds.","Move the specific synchronization worker to a containerized environment (e.g., AWS Fargate or Fly.io) if recursive serverless calls prove too costly or unreliable.","Vercel function logs showing 'Task timed out' errors or QStash Dead Letter Queue growing > 5 messages.","Lead Backend Engineer","2025-06-15","Not Started"
"RISK-002","External","Salla API Rate Limiting triggering cascading failures during bulk ingestion. Aggressive historical syncing combined with real-time webhooks may trigger 429 errors, blocking legitimate traffic.","5","4","20","High","WI-DATA-001, WI-DATA-002, WI-DATA-003","External dependency on Salla API throughput limits which are outside platform control.","Implement strict rate limiting middleware using Upstash Redis (Leaky Bucket algorithm) to throttle outgoing requests. Develop exponential backoff logic for 429 responses within the worker.","Reduce historical sync scope automatically (e.g., fetch only last 3 months initially) if rate limits are hit, loading older data in background trickle mode.","Log aggregation showing > 5% of outgoing requests returning HTTP 429.","Integration Specialist","2025-06-20","Not Started"
"RISK-003","Technical","PostgreSQL connection exhaustion due to Prisma/Serverless architecture. High concurrency from webhooks and dashboard loads can deplete the connection pool in a serverless environment.","4","5","20","Critical","WI-INFRA-002, WI-AUTH-001, WI-DATA-003","Serverless functions create new database connections per invocation rather than maintaining a persistent pool.","Mandatory configuration of PgBouncer (connection pooling) for the production database. Configure Prisma to use the connection pool string for the application and direct connection only for migrations.","Migrate from standard Postgres to a serverless-native database offering like Neon or PlanetScale that handles connection pooling at the platform level.","Database monitoring showing 'active_connections' reaching 80% of max limit.","DevOps Engineer","2025-06-01","In Progress"
"RISK-004","Technical","Data drift between PostgreSQL (OLTP) and ClickHouse (OLAP). The async CDC pipeline via QStash may drop messages or process out of order, leading to inaccurate analytics reports.","4","4","16","High","WI-DATA-004, WS-CDC-OLAP","Eventual consistency model and lack of transactional integrity between two distinct database systems.","Implement an 'Idempotent Upsert' pattern in ClickHouse using ReplacingMergeTree. Develop a nightly reconciliation job that compares row counts and checksums between Postgres and ClickHouse.","Implement a 'Full Replay' button in the admin panel to truncate and reload ClickHouse data for a specific tenant from Postgres.","Automated data quality check fails > 1% discrepancy in Daily Sales value between OLTP and OLAP.","Data Engineer","2025-07-01","Not Started"
"RISK-005","Security","Cross-tenant data leakage via RBAC Middleware bypass. If the `merchant_id` context is not strictly enforced in every Prisma query, users might access competitor data.","3","5","15","High","WI-AUTH-003, WI-ANA-001","Reliance on application-level logic for multitenancy rather than database-level Row Level Security (RLS).","Implement a Prisma extension or custom client wrapper that strictly requires `merchant_id` injection for every query. Write integration tests specifically attempting to access cross-tenant resources.","Emergency rollback and implementation of Postgres RLS (Row Level Security) policies if application-level isolation proves porous.","Security audit logs showing access attempts to resources with mismatched `merchant_id`.","Security Engineer","2025-06-10","Not Started"
"RISK-006","Operational","Analytical query performance degrading beyond 200ms. Poorly optimized ClickHouse partition keys or sorting keys causing full table scans as data volume grows.","3","3","9","Medium","WI-INFRA-003, WI-ANA-001","Suboptimal schema design in ClickHouse relative to the actual query patterns used by the dashboards.","Define ClickHouse sorting keys based on `(MerchantID, EventDate)`. Use Materialized Views for pre-aggregating common KPI metrics (Sales, Orders) to avoid computing on raw data.","Increase ClickHouse compute resource tier temporarily while optimizing queries.","Vercel Analytics showing p95 API latency for `/api/analytics` endpoints exceeding 300ms.","Data Engineer","2025-06-05","Not Started"
"RISK-007","Technical","AI RAG pipeline latency exceeding 5 seconds (WI-AI-002). Multiple hops (Next.js -> Vector DB -> OpenAI -> ClickHouse) introduce compounding latency, leading to poor UX.","4","2","8","Medium","WI-AI-001, WI-AI-002","Synchronous chaining of slow external API calls (OpenAI) and database queries.","Implement streaming responses (Server Sent Events) for the AI chat interface to provide immediate feedback. Cache frequent NLQ-to-SQL translations.","Switch to a smaller/faster model (e.g., gpt-3.5-turbo) instead of gpt-4 for query generation if latency is unacceptable.","Axiom logs showing average response time for `/api/ai/query` > 5000ms.","AI/ML Engineer","2025-07-15","Not Started"